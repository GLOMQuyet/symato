{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dowload github and the necessary libraries to train the model"
      ],
      "metadata": {
        "id": "UVjUBZz_7Qg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh5Iwk2ROPOb",
        "outputId": "00c50e57-4bed-46d3-fcea-0f6672970cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'symato'...\n",
            "remote: Enumerating objects: 1145, done.\u001b[K\n",
            "remote: Counting objects: 100% (577/577), done.\u001b[K\n",
            "remote: Compressing objects: 100% (277/277), done.\u001b[K\n",
            "remote: Total 1145 (delta 345), reused 500 (delta 296), pack-reused 568\u001b[K\n",
            "Receiving objects: 100% (1145/1145), 19.30 MiB | 14.41 MiB/s, done.\n",
            "Resolving deltas: 100% (644/644), done.\n",
            "/content/symato\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepspeed==0.8.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: pytorch_lightning==1.9.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: torch==1.13.1+cu116 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.13.1+cu116)\n",
            "Requirement already satisfied: transformers==4.26.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.26.0)\n",
            "Requirement already satisfied: wandb==0.13.10 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (0.13.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (4.64.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (1.10.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.4.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (0.6.0.post0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (2023.1.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 5)) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 5)) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 5)) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 5)) (2.25.1)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (3.1.31)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.10->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb==0.13.10->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (3.8.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb==0.13.10->-r requirements.txt (line 6)) (4.0.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 5)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 5)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 5)) (1.26.14)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.13.10->-r requirements.txt (line 6)) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GLOMQuyet/symato.git\n",
        "%cd /content/symato\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dowload and unzip dataset public from discord"
      ],
      "metadata": {
        "id": "zu9yV9Qb7mr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6u5Rg5zQI3z",
        "outputId": "f5a720ad-8648-41e0-8f32-8c8c7db0df0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/symato/rwkv-v4neo\n",
            "--2023-02-20 13:12:23--  https://cdn.discordapp.com/attachments/1070553582111297616/1071335804451954688/vlc.zip\n",
            "Resolving cdn.discordapp.com (cdn.discordapp.com)... 162.159.130.233, 162.159.134.233, 162.159.133.233, ...\n",
            "Connecting to cdn.discordapp.com (cdn.discordapp.com)|162.159.130.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4738750 (4.5M) [application/zip]\n",
            "Saving to: ‘vlc.zip’\n",
            "\n",
            "vlc.zip             100%[===================>]   4.52M  3.50MB/s    in 1.3s    \n",
            "\n",
            "2023-02-20 13:12:25 (3.50 MB/s) - ‘vlc.zip’ saved [4738750/4738750]\n",
            "\n",
            "Archive:  vlc.zip\n",
            "  inflating: vlc.txt                 \n",
            "  inflating: vlc.xyz                 \n"
          ]
        }
      ],
      "source": [
        "%cd rwkv-v4neo\n",
        "!wget https://cdn.discordapp.com/attachments/1070553582111297616/1071335804451954688/vlc.zip\n",
        "!unzip vlc.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Tiny model"
      ],
      "metadata": {
        "id": "KHWzXGRM7vis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjgbCZ-xOgd1",
        "outputId": "ef4dd79c-1fea-4e08-cd93-ec3777ea4c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 13:12:33.000006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 13:12:34.195567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:12:34.195686: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:12:34.195708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "########## work in progress ##########\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# RWKV-4 FP16 on 1x1 GPU, bsz 1x1x128=128, ddp_find_unused_parameters_false \n",
            "#\n",
            "# Data = vlc.xyz (symato), ProjDir = out\n",
            "#\n",
            "# Epoch = 0 to 19 (will continue afterwards), save every 5 epoch\n",
            "#\n",
            "# Each \"epoch\" = 2000 steps, 256000 samples, 131072000 tokens\n",
            "#\n",
            "# Model = 4 n_layer, 320 n_embd, 512 ctx_len\n",
            "#\n",
            "# Adam = lr 0.0008 to 1e-06, warmup 0 steps, beta (0.9, 0.99), eps 1e-08\n",
            "#\n",
            "# Found torch 1.13.1+cu116, recommend 1.12.1+cu116 or newer\n",
            "# Found deepspeed 0.8.0, recommend 0.7.0 (faster than newer versions)\n",
            "# Found pytorch_lightning 1.9.0, recommend 1.7.4 or newer\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "{'data_order': '', 'load_model': '', 'wandb': '', 'proj_dir': 'out', 'random_seed': -1, 'data_file': 'vlc.xyz', 'data_type': 'symato', 'vocab_size': 0, 'ctx_len': 512, 'epoch_steps': 2000, 'epoch_count': 20, 'epoch_begin': 0, 'epoch_save': 5, 'micro_bsz': 128, 'n_layer': 4, 'n_embd': 320, 'pre_ffn': 0, 'head_qk': 0, 'tiny_att_dim': 0, 'tiny_att_layer': -999, 'lr_init': 0.0008, 'lr_final': 1e-06, 'warmup_steps': 0, 'beta1': 0.9, 'beta2': 0.99, 'adam_eps': 1e-08, 'grad_cp': 0, 'my_pile_stage': 0, 'my_pile_shift': -1, 'my_pile_edecay': 0, 'layerwise_lr': 1, 'ds_bucket_mb': 200, 'my_sample_len': 0, 'my_ffn_shift': 1, 'my_att_shift': 1, 'my_pos_emb': 0, 'load_partial': 0, 'magic_prime': 0, 'logger': False, 'enable_checkpointing': False, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': None, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 100000000000000000000, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': -1, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 100000000000000000000, 'accelerator': 'gpu', 'strategy': 'ddp_find_unused_parameters_false', 'sync_batchnorm': False, 'precision': 'fp16', 'enable_model_summary': True, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': None, 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True, 'my_timestamp': '2023-02-20-13-12-42', 'betas': (0.9, 0.99), 'real_bsz': 128, 'run_name': '0 ctx512 L4 D320'}\n",
            "\n",
            "\n",
            "\n",
            "Note: you are using fp16 (might overflow). Try bf16 / tf32 for stable training.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "- - - [ TRAIN DATA SAMPLE ] - - -\n",
            " hanh|f \n",
            "\n",
            "^ddieu|zf 53. ^hieu|zj luc|wj thi| hanh|f \n",
            "\n",
            "^luat|zj nay|f co|s hieu|zj luc|wj thi| hanh|f tu|wf ngay|f 01 thang|s 7 nam|w 2013 va|f thay| the|zs ^luat|zj xuat|zs ban|r so|zs 30/2004/QH11 dda|x dduoc|wj sua|wr ddoi|zr, bo|zr sung| mot|zj so|zs ddieu|zf theo| ^luat|zj so|zs 12/2008/QH12. \n",
            "\n",
            "^ddieu|zf 54. ^quy| ddinh|j chi| tiet|zs va|f huong|ws dan|zx thi| hanh|f \n",
            "\n",
            "^chinh|s phu|r, co|w quan| co|s tham|zr quyen|zf quy| ddinh|j chi| tiet|zs, huong|ws dan|zx thi| hanh|f cac|s ddieu|zf, khoan|r dduoc|wj giao| trong| ^luat|zj. \n",
            "\n",
            "^luat|zj nay|f dda|x dduoc|wj ^quoc|zs hoi|zj nuoc|ws ^cong|zj hoa|f xa|x hoi|zj chu|r nghia|x ^viet|zj ^nam| khoa|s XIII, ky|f hop|j thu|ws 4 thong|z qua| ngay|f 20 thang|s 11 nam|w 2012. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "^^chu|r ^^tich|j ^^quoc|zs ^^hoi|zj \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "^nguyen|zx ^sinh| ^hung|f \n",
            " \n",
            "\n",
            "\n",
            "Current vocab size = 2944 (make sure it's correct)\n",
            "Data has 16318403 samples.\n",
            "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu116/wkv_512/build.ninja...\n",
            "Building extension module wkv_512...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 -DTmax=512 -std=c++14 -c /content/symato/rwkv-v4neo/wkv_cuda.cu -o wkv_cuda.cuda.o \n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_PS0_S3_S3_S3_\n",
            "    6144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 43 registers, 440 bytes cmem[0], 8 bytes cmem[2]\n",
            "ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 40 registers, 408 bytes cmem[0]\n",
            "[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_512 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /content/symato/rwkv-v4neo/wkv_op.cpp -o wkv_op.o \n",
            "[3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512.so\n",
            "Loading extension module wkv_512...\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# Init model weight (slow for large models)...\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "2944  320   -0.0008 emb.weight\n",
            "320   320   0    blocks.0.att.key.weight\n",
            "320   320   1.0  blocks.0.att.value.weight\n",
            "320   320   0    blocks.0.att.receptance.weight\n",
            "320   320   0    blocks.0.att.output.weight\n",
            "1280  320   1.0  blocks.0.ffn.key.weight\n",
            "320   320   0    blocks.0.ffn.receptance.weight\n",
            "320   1280  0    blocks.0.ffn.value.weight\n",
            "320   320   0    blocks.1.att.key.weight\n",
            "320   320   1.0  blocks.1.att.value.weight\n",
            "320   320   0    blocks.1.att.receptance.weight\n",
            "320   320   0    blocks.1.att.output.weight\n",
            "1280  320   1.0  blocks.1.ffn.key.weight\n",
            "320   320   0    blocks.1.ffn.receptance.weight\n",
            "320   1280  0    blocks.1.ffn.value.weight\n",
            "320   320   0    blocks.2.att.key.weight\n",
            "320   320   1.0  blocks.2.att.value.weight\n",
            "320   320   0    blocks.2.att.receptance.weight\n",
            "320   320   0    blocks.2.att.output.weight\n",
            "1280  320   1.0  blocks.2.ffn.key.weight\n",
            "320   320   0    blocks.2.ffn.receptance.weight\n",
            "320   1280  0    blocks.2.ffn.value.weight\n",
            "320   320   0    blocks.3.att.key.weight\n",
            "320   320   1.0  blocks.3.att.value.weight\n",
            "320   320   0    blocks.3.att.receptance.weight\n",
            "320   320   0    blocks.3.att.output.weight\n",
            "1280  320   1.0  blocks.3.ffn.key.weight\n",
            "320   320   0    blocks.3.ffn.receptance.weight\n",
            "320   1280  0    blocks.3.ffn.value.weight\n",
            "2944  320   0.5  head.weight\n",
            "Save to out/rwkv-init.pth...\n",
            "########## Loading out/rwkv-init.pth... ##########\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "2944  320   emb.weight\n",
            "320         blocks.0.ln1.weight\n",
            "320         blocks.0.ln1.bias\n",
            "320         blocks.0.ln2.weight\n",
            "320         blocks.0.ln2.bias\n",
            "320         blocks.0.ln0.weight\n",
            "320         blocks.0.ln0.bias\n",
            "320         blocks.0.att.time_decay\n",
            "320         blocks.0.att.time_first\n",
            "320         blocks.0.att.time_mix_k\n",
            "320         blocks.0.att.time_mix_v\n",
            "320         blocks.0.att.time_mix_r\n",
            "320   320   blocks.0.att.key.weight\n",
            "320   320   blocks.0.att.value.weight\n",
            "320   320   blocks.0.att.receptance.weight\n",
            "320   320   blocks.0.att.output.weight\n",
            "320         blocks.0.ffn.time_mix_k\n",
            "320         blocks.0.ffn.time_mix_r\n",
            "1280  320   blocks.0.ffn.key.weight\n",
            "320   320   blocks.0.ffn.receptance.weight\n",
            "320   1280  blocks.0.ffn.value.weight\n",
            "320         blocks.1.ln1.weight\n",
            "320         blocks.1.ln1.bias\n",
            "320         blocks.1.ln2.weight\n",
            "320         blocks.1.ln2.bias\n",
            "320         blocks.1.att.time_decay\n",
            "320         blocks.1.att.time_first\n",
            "320         blocks.1.att.time_mix_k\n",
            "320         blocks.1.att.time_mix_v\n",
            "320         blocks.1.att.time_mix_r\n",
            "320   320   blocks.1.att.key.weight\n",
            "320   320   blocks.1.att.value.weight\n",
            "320   320   blocks.1.att.receptance.weight\n",
            "320   320   blocks.1.att.output.weight\n",
            "320         blocks.1.ffn.time_mix_k\n",
            "320         blocks.1.ffn.time_mix_r\n",
            "1280  320   blocks.1.ffn.key.weight\n",
            "320   320   blocks.1.ffn.receptance.weight\n",
            "320   1280  blocks.1.ffn.value.weight\n",
            "320         blocks.2.ln1.weight\n",
            "320         blocks.2.ln1.bias\n",
            "320         blocks.2.ln2.weight\n",
            "320         blocks.2.ln2.bias\n",
            "320         blocks.2.att.time_decay\n",
            "320         blocks.2.att.time_first\n",
            "320         blocks.2.att.time_mix_k\n",
            "320         blocks.2.att.time_mix_v\n",
            "320         blocks.2.att.time_mix_r\n",
            "320   320   blocks.2.att.key.weight\n",
            "320   320   blocks.2.att.value.weight\n",
            "320   320   blocks.2.att.receptance.weight\n",
            "320   320   blocks.2.att.output.weight\n",
            "320         blocks.2.ffn.time_mix_k\n",
            "320         blocks.2.ffn.time_mix_r\n",
            "1280  320   blocks.2.ffn.key.weight\n",
            "320   320   blocks.2.ffn.receptance.weight\n",
            "320   1280  blocks.2.ffn.value.weight\n",
            "320         blocks.3.ln1.weight\n",
            "320         blocks.3.ln1.bias\n",
            "320         blocks.3.ln2.weight\n",
            "320         blocks.3.ln2.bias\n",
            "320         blocks.3.att.time_decay\n",
            "320         blocks.3.att.time_first\n",
            "320         blocks.3.att.time_mix_k\n",
            "320         blocks.3.att.time_mix_v\n",
            "320         blocks.3.att.time_mix_r\n",
            "320   320   blocks.3.att.key.weight\n",
            "320   320   blocks.3.att.value.weight\n",
            "320   320   blocks.3.att.receptance.weight\n",
            "320   320   blocks.3.att.output.weight\n",
            "320         blocks.3.ffn.time_mix_k\n",
            "320         blocks.3.ffn.time_mix_r\n",
            "1280  320   blocks.3.ffn.key.weight\n",
            "320   320   blocks.3.ffn.receptance.weight\n",
            "320   1280  blocks.3.ffn.value.weight\n",
            "320         ln_out.weight\n",
            "320         ln_out.bias\n",
            "2944  320   head.weight\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type       | Params\n",
            "--------------------------------------\n",
            "0 | emb    | Embedding  | 942 K \n",
            "1 | blocks | ModuleList | 5.3 M \n",
            "2 | ln_out | LayerNorm  | 640   \n",
            "3 | head   | Linear     | 942 K \n",
            "--------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "14.449    Total estimated model params size (MB)\n",
            "Epoch 0:   4% 77/2000 [00:38<16:01,  2.00it/s, loss=1.610, lr=0.00079, REAL it/s=2.200, Kt/s=144.0]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "!python3 train.py --load_model \"\" --wandb \"\" --proj_dir \"out\" \\\n",
        "--data_file \"vlc.xyz\" --data_type \"symato\" --vocab_size 0 \\\n",
        "--ctx_len 512 --epoch_steps 2000 --epoch_count 20 --epoch_begin 0 --epoch_save 5 \\\n",
        "--micro_bsz 128 --n_layer 4 --n_embd 320 --pre_ffn 0 --head_qk 0 \\\n",
        "--lr_init 8e-4 --lr_final 1e-6 --warmup_steps 0 --beta1 0.9 --beta2 0.99 \\\n",
        "--accelerator gpu --devices 1 --precision fp16 --strategy ddp_find_unused_parameters_false --grad_cp 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Base model"
      ],
      "metadata": {
        "id": "X_ZjhoQl8Bwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --data_order=reversed --load_model \"\" --wandb \"\" --proj_dir \"out\" \\\n",
        "--data_file \"vlc.xyz\" --data_type \"symato\" \\\n",
        "--ctx_len 640 --epoch_steps 2000 --epoch_count 20 --epoch_begin 0 --epoch_save 5 \\\n",
        "--micro_bsz 64 --n_layer 6 --n_embd 512 --pre_ffn 0 --head_qk 0 \\\n",
        "--lr_init 8e-4 --lr_final 1e-6 --warmup_steps 0 --beta1 0.9 --beta2 0.99 \\\n",
        "--accelerator gpu --devices 1 --precision fp16 --strategy ddp_find_unused_parameters_false"
      ],
      "metadata": {
        "id": "z5P-PT6NQM7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fc8cba-1a3d-45d6-ddf4-4c63a8cd1491"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 13:15:49.503252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 13:15:50.395912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:15:50.396030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:15:50.396052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "########## work in progress ##########\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# RWKV-4 FP16 on 1x1 GPU, bsz 1x1x64=64, ddp_find_unused_parameters_false \n",
            "#\n",
            "# Data = vlc.xyz (symato), ProjDir = out\n",
            "#\n",
            "# Epoch = 0 to 19 (will continue afterwards), save every 5 epoch\n",
            "#\n",
            "# Each \"epoch\" = 2000 steps, 128000 samples, 81920000 tokens\n",
            "#\n",
            "# Model = 6 n_layer, 512 n_embd, 640 ctx_len\n",
            "#\n",
            "# Adam = lr 0.0008 to 1e-06, warmup 0 steps, beta (0.9, 0.99), eps 1e-08\n",
            "#\n",
            "# Found torch 1.13.1+cu116, recommend 1.12.1+cu116 or newer\n",
            "# Found deepspeed 0.8.0, recommend 0.7.0 (faster than newer versions)\n",
            "# Found pytorch_lightning 1.9.0, recommend 1.7.4 or newer\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "{'data_order': 'reversed', 'load_model': '', 'wandb': '', 'proj_dir': 'out', 'random_seed': -1, 'data_file': 'vlc.xyz', 'data_type': 'symato', 'vocab_size': 0, 'ctx_len': 640, 'epoch_steps': 2000, 'epoch_count': 20, 'epoch_begin': 0, 'epoch_save': 5, 'micro_bsz': 64, 'n_layer': 6, 'n_embd': 512, 'pre_ffn': 0, 'head_qk': 0, 'tiny_att_dim': 0, 'tiny_att_layer': -999, 'lr_init': 0.0008, 'lr_final': 1e-06, 'warmup_steps': 0, 'beta1': 0.9, 'beta2': 0.99, 'adam_eps': 1e-08, 'grad_cp': 0, 'my_pile_stage': 0, 'my_pile_shift': -1, 'my_pile_edecay': 0, 'layerwise_lr': 1, 'ds_bucket_mb': 200, 'my_sample_len': 0, 'my_ffn_shift': 1, 'my_att_shift': 1, 'my_pos_emb': 0, 'load_partial': 0, 'magic_prime': 0, 'logger': False, 'enable_checkpointing': False, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': None, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 100000000000000000000, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': -1, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 100000000000000000000, 'accelerator': 'gpu', 'strategy': 'ddp_find_unused_parameters_false', 'sync_batchnorm': False, 'precision': 'fp16', 'enable_model_summary': True, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': None, 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True, 'my_timestamp': '2023-02-20-13-15-56', 'betas': (0.9, 0.99), 'real_bsz': 64, 'run_name': '0 ctx640 L6 D512'}\n",
            "\n",
            "\n",
            "\n",
            "Note: you are using fp16 (might overflow). Try bf16 / tf32 for stable training.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "- - - [ TRAIN DATA SAMPLE ] - - -\n",
            "  r|nas f|iat f|av z|naht z|nahn fz|ev j|uv x|aihgn ,fz|neyuq ;z|nahn s|pahp ,z|nahn s|ac r|auc rw|ux sw|gnu s|hcac fz|ev s|yl s|pahp jw|cum rz|nauhc ,s|yl s|pahp j|iv j|aidd j|hnidd |yuq f|yan jz|taul jz|ob^\n",
            "\n",
            " r|hnihc fz|ueidd |iv j|mahp^ .1 fz|ueidd^\n",
            "\n",
            " |gnuhc^^ j|hnidd^^ |yuq^^ xw|gnuhn^^\n",
            "\n",
            " |i^ w|gnouhc^\n",
            "\n",
            " |gnuhc^^ j|hnidd^^ |yuq^^\n",
            "\n",
            " sz|tahn sw|uht fz|nahp^\n",
            "\n",
            " .jw|us z|nad jz|taul jz|ob^ f|hnah |nab jz|ioh sz|couq^\n",
            "\n",
            " ;|man^ jz|teiv^ x|aihgn r|uhc jz|ioh x|ax f|aoh jz|gnoc^ sw|coun s|pahp sz|neih^ sw|uc w|nac^\n",
            "\n",
            " jw|us^^ z|nad^^\n",
            "\n",
            " jz|taul^^ jz|ob^^\n",
            "\n",
            "\n",
            "\n",
            " 5102 w|man 11 s|gnaht 42 f|yagn ,jz|ion^ f|ah^\n",
            "\n",
            " 31HQ/5102/19 :sz|os jz|taul^\n",
            "\n",
            " ---------------\n",
            " s|cuhp j|hnah^ - |od jw|ut^ - jz|pal jz|codd^\n",
            " |man^^ jz|teiv^^ x|aihgn^^ r|uhc^^ jz|ioh^^ x|ax^^ f|aoh^^ jz|gnoc^^\n",
            "\n",
            " --------\n",
            " jz|ioh^^ sz|couq^^ \n",
            "\n",
            "\n",
            "Current vocab size = 2944 (make sure it's correct)\n",
            "Data has 16318403 samples.\n",
            "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu116/wkv_640/build.ninja...\n",
            "Building extension module wkv_640...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module wkv_640...\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# Init model weight (slow for large models)...\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "2944  512   -0.0008 emb.weight\n",
            "512   512   0    blocks.0.att.key.weight\n",
            "512   512   1.0  blocks.0.att.value.weight\n",
            "512   512   0    blocks.0.att.receptance.weight\n",
            "512   512   0    blocks.0.att.output.weight\n",
            "2048  512   1.0  blocks.0.ffn.key.weight\n",
            "512   512   0    blocks.0.ffn.receptance.weight\n",
            "512   2048  0    blocks.0.ffn.value.weight\n",
            "512   512   0    blocks.1.att.key.weight\n",
            "512   512   1.0  blocks.1.att.value.weight\n",
            "512   512   0    blocks.1.att.receptance.weight\n",
            "512   512   0    blocks.1.att.output.weight\n",
            "2048  512   1.0  blocks.1.ffn.key.weight\n",
            "512   512   0    blocks.1.ffn.receptance.weight\n",
            "512   2048  0    blocks.1.ffn.value.weight\n",
            "512   512   0    blocks.2.att.key.weight\n",
            "512   512   1.0  blocks.2.att.value.weight\n",
            "512   512   0    blocks.2.att.receptance.weight\n",
            "512   512   0    blocks.2.att.output.weight\n",
            "2048  512   1.0  blocks.2.ffn.key.weight\n",
            "512   512   0    blocks.2.ffn.receptance.weight\n",
            "512   2048  0    blocks.2.ffn.value.weight\n",
            "512   512   0    blocks.3.att.key.weight\n",
            "512   512   1.0  blocks.3.att.value.weight\n",
            "512   512   0    blocks.3.att.receptance.weight\n",
            "512   512   0    blocks.3.att.output.weight\n",
            "2048  512   1.0  blocks.3.ffn.key.weight\n",
            "512   512   0    blocks.3.ffn.receptance.weight\n",
            "512   2048  0    blocks.3.ffn.value.weight\n",
            "512   512   0    blocks.4.att.key.weight\n",
            "512   512   1.0  blocks.4.att.value.weight\n",
            "512   512   0    blocks.4.att.receptance.weight\n",
            "512   512   0    blocks.4.att.output.weight\n",
            "2048  512   1.0  blocks.4.ffn.key.weight\n",
            "512   512   0    blocks.4.ffn.receptance.weight\n",
            "512   2048  0    blocks.4.ffn.value.weight\n",
            "512   512   0    blocks.5.att.key.weight\n",
            "512   512   1.0  blocks.5.att.value.weight\n",
            "512   512   0    blocks.5.att.receptance.weight\n",
            "512   512   0    blocks.5.att.output.weight\n",
            "2048  512   1.0  blocks.5.ffn.key.weight\n",
            "512   512   0    blocks.5.ffn.receptance.weight\n",
            "512   2048  0    blocks.5.ffn.value.weight\n",
            "2944  512   0.5  head.weight\n",
            "Save to out/rwkv-init.pth...\n",
            "########## Loading out/rwkv-init.pth... ##########\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "2944  512   emb.weight\n",
            "512         blocks.0.ln1.weight\n",
            "512         blocks.0.ln1.bias\n",
            "512         blocks.0.ln2.weight\n",
            "512         blocks.0.ln2.bias\n",
            "512         blocks.0.ln0.weight\n",
            "512         blocks.0.ln0.bias\n",
            "512         blocks.0.att.time_decay\n",
            "512         blocks.0.att.time_first\n",
            "512         blocks.0.att.time_mix_k\n",
            "512         blocks.0.att.time_mix_v\n",
            "512         blocks.0.att.time_mix_r\n",
            "512   512   blocks.0.att.key.weight\n",
            "512   512   blocks.0.att.value.weight\n",
            "512   512   blocks.0.att.receptance.weight\n",
            "512   512   blocks.0.att.output.weight\n",
            "512         blocks.0.ffn.time_mix_k\n",
            "512         blocks.0.ffn.time_mix_r\n",
            "2048  512   blocks.0.ffn.key.weight\n",
            "512   512   blocks.0.ffn.receptance.weight\n",
            "512   2048  blocks.0.ffn.value.weight\n",
            "512         blocks.1.ln1.weight\n",
            "512         blocks.1.ln1.bias\n",
            "512         blocks.1.ln2.weight\n",
            "512         blocks.1.ln2.bias\n",
            "512         blocks.1.att.time_decay\n",
            "512         blocks.1.att.time_first\n",
            "512         blocks.1.att.time_mix_k\n",
            "512         blocks.1.att.time_mix_v\n",
            "512         blocks.1.att.time_mix_r\n",
            "512   512   blocks.1.att.key.weight\n",
            "512   512   blocks.1.att.value.weight\n",
            "512   512   blocks.1.att.receptance.weight\n",
            "512   512   blocks.1.att.output.weight\n",
            "512         blocks.1.ffn.time_mix_k\n",
            "512         blocks.1.ffn.time_mix_r\n",
            "2048  512   blocks.1.ffn.key.weight\n",
            "512   512   blocks.1.ffn.receptance.weight\n",
            "512   2048  blocks.1.ffn.value.weight\n",
            "512         blocks.2.ln1.weight\n",
            "512         blocks.2.ln1.bias\n",
            "512         blocks.2.ln2.weight\n",
            "512         blocks.2.ln2.bias\n",
            "512         blocks.2.att.time_decay\n",
            "512         blocks.2.att.time_first\n",
            "512         blocks.2.att.time_mix_k\n",
            "512         blocks.2.att.time_mix_v\n",
            "512         blocks.2.att.time_mix_r\n",
            "512   512   blocks.2.att.key.weight\n",
            "512   512   blocks.2.att.value.weight\n",
            "512   512   blocks.2.att.receptance.weight\n",
            "512   512   blocks.2.att.output.weight\n",
            "512         blocks.2.ffn.time_mix_k\n",
            "512         blocks.2.ffn.time_mix_r\n",
            "2048  512   blocks.2.ffn.key.weight\n",
            "512   512   blocks.2.ffn.receptance.weight\n",
            "512   2048  blocks.2.ffn.value.weight\n",
            "512         blocks.3.ln1.weight\n",
            "512         blocks.3.ln1.bias\n",
            "512         blocks.3.ln2.weight\n",
            "512         blocks.3.ln2.bias\n",
            "512         blocks.3.att.time_decay\n",
            "512         blocks.3.att.time_first\n",
            "512         blocks.3.att.time_mix_k\n",
            "512         blocks.3.att.time_mix_v\n",
            "512         blocks.3.att.time_mix_r\n",
            "512   512   blocks.3.att.key.weight\n",
            "512   512   blocks.3.att.value.weight\n",
            "512   512   blocks.3.att.receptance.weight\n",
            "512   512   blocks.3.att.output.weight\n",
            "512         blocks.3.ffn.time_mix_k\n",
            "512         blocks.3.ffn.time_mix_r\n",
            "2048  512   blocks.3.ffn.key.weight\n",
            "512   512   blocks.3.ffn.receptance.weight\n",
            "512   2048  blocks.3.ffn.value.weight\n",
            "512         blocks.4.ln1.weight\n",
            "512         blocks.4.ln1.bias\n",
            "512         blocks.4.ln2.weight\n",
            "512         blocks.4.ln2.bias\n",
            "512         blocks.4.att.time_decay\n",
            "512         blocks.4.att.time_first\n",
            "512         blocks.4.att.time_mix_k\n",
            "512         blocks.4.att.time_mix_v\n",
            "512         blocks.4.att.time_mix_r\n",
            "512   512   blocks.4.att.key.weight\n",
            "512   512   blocks.4.att.value.weight\n",
            "512   512   blocks.4.att.receptance.weight\n",
            "512   512   blocks.4.att.output.weight\n",
            "512         blocks.4.ffn.time_mix_k\n",
            "512         blocks.4.ffn.time_mix_r\n",
            "2048  512   blocks.4.ffn.key.weight\n",
            "512   512   blocks.4.ffn.receptance.weight\n",
            "512   2048  blocks.4.ffn.value.weight\n",
            "512         blocks.5.ln1.weight\n",
            "512         blocks.5.ln1.bias\n",
            "512         blocks.5.ln2.weight\n",
            "512         blocks.5.ln2.bias\n",
            "512         blocks.5.att.time_decay\n",
            "512         blocks.5.att.time_first\n",
            "512         blocks.5.att.time_mix_k\n",
            "512         blocks.5.att.time_mix_v\n",
            "512         blocks.5.att.time_mix_r\n",
            "512   512   blocks.5.att.key.weight\n",
            "512   512   blocks.5.att.value.weight\n",
            "512   512   blocks.5.att.receptance.weight\n",
            "512   512   blocks.5.att.output.weight\n",
            "512         blocks.5.ffn.time_mix_k\n",
            "512         blocks.5.ffn.time_mix_r\n",
            "2048  512   blocks.5.ffn.key.weight\n",
            "512   512   blocks.5.ffn.receptance.weight\n",
            "512   2048  blocks.5.ffn.value.weight\n",
            "512         ln_out.weight\n",
            "512         ln_out.bias\n",
            "2944  512   head.weight\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type       | Params\n",
            "--------------------------------------\n",
            "0 | emb    | Embedding  | 1.5 M \n",
            "1 | blocks | ModuleList | 20.5 M\n",
            "2 | ln_out | LayerNorm  | 1.0 K \n",
            "3 | head   | Linear     | 1.5 M \n",
            "--------------------------------------\n",
            "23.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "23.5 M    Total params\n",
            "46.995    Total estimated model params size (MB)\n",
            "Epoch 0:   1% 27/2000 [00:21<26:10,  1.26it/s, loss=2.320, lr=0.000796, REAL it/s=1.510, Kt/s=62.00]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with tokenizer byte2byte from Phobert"
      ],
      "metadata": {
        "id": "cNnpJGIm8P2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir 'data'"
      ],
      "metadata": {
        "id": "ffqXBe2g_vBb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data\n",
        "!wget https://huggingface.co/vinai/phobert-base/raw/main/vocab.txt;\n",
        "!wget https://huggingface.co/vinai/phobert-base/raw/main/tokenizer.json;\n",
        "!wget https://huggingface.co/vinai/phobert-base/raw/main/bpe.codes;\n",
        "%cd /content/symato/rwkv-v4neo\n",
        "!python3 train.py --load_model \"\" --wandb \"pho-vlc\" --proj_dir \"out\" \\\n",
        "--data_file \"vlc.txt\" --data_type \"utf-8\" \\\n",
        "--ctx_len 192 --epoch_steps 2000 --epoch_count 20 --epoch_begin 0 --epoch_save 5 \\\n",
        "--micro_bsz 64 --n_layer 5 --n_embd 320 --pre_ffn 0 --head_qk 0 \\\n",
        "--lr_init 8e-4 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.99\\\n",
        "--accelerator gpu --devices 1 --precision fp16 --strategy ddp_find_unused_parameters_false --grad_cp 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d0O63bJ8Y9T",
        "outputId": "aab1bcf9-501d-4dd6-a731-afa3a1280981"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/symato/rwkv-v4neo/data\n",
            "--2023-02-20 13:24:31--  https://huggingface.co/vinai/phobert-base/raw/main/vocab.txt\n",
            "Resolving huggingface.co (huggingface.co)... 3.231.67.228, 54.235.118.239, 2600:1f18:147f:e850:e203:c458:10cd:fc3c, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.231.67.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895321 (874K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 874.34K   790KB/s    in 1.1s    \n",
            "\n",
            "2023-02-20 13:24:33 (790 KB/s) - ‘vocab.txt’ saved [895321/895321]\n",
            "\n",
            "--2023-02-20 13:24:33--  https://huggingface.co/vinai/phobert-base/raw/main/tokenizer.json\n",
            "Resolving huggingface.co (huggingface.co)... 3.231.67.228, 54.235.118.239, 2600:1f18:147f:e850:e203:c458:10cd:fc3c, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.231.67.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3132320 (3.0M) [text/plain]\n",
            "Saving to: ‘tokenizer.json’\n",
            "\n",
            "tokenizer.json      100%[===================>]   2.99M  2.22MB/s    in 1.3s    \n",
            "\n",
            "2023-02-20 13:24:36 (2.22 MB/s) - ‘tokenizer.json’ saved [3132320/3132320]\n",
            "\n",
            "--2023-02-20 13:24:36--  https://huggingface.co/vinai/phobert-base/raw/main/bpe.codes\n",
            "Resolving huggingface.co (huggingface.co)... 54.235.118.239, 3.231.67.228, 2600:1f18:147f:e850:e203:c458:10cd:fc3c, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.235.118.239|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1135173 (1.1M) [text/plain]\n",
            "Saving to: ‘bpe.codes’\n",
            "\n",
            "bpe.codes           100%[===================>]   1.08M   833KB/s    in 1.3s    \n",
            "\n",
            "2023-02-20 13:24:38 (833 KB/s) - ‘bpe.codes’ saved [1135173/1135173]\n",
            "\n",
            "/content/symato/rwkv-v4neo\n",
            "2023-02-20 13:24:40.984849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 13:24:41.883431: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:24:41.883559: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 13:24:41.883579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "########## work in progress ##########\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# RWKV-4 FP16 on 1x1 GPU, bsz 1x1x64=64, ddp_find_unused_parameters_false \n",
            "#\n",
            "# Data = vlc.txt (utf-8), ProjDir = out\n",
            "#\n",
            "# Epoch = 0 to 19 (will continue afterwards), save every 5 epoch\n",
            "#\n",
            "# Each \"epoch\" = 2000 steps, 128000 samples, 24576000 tokens\n",
            "#\n",
            "# Model = 5 n_layer, 320 n_embd, 192 ctx_len\n",
            "#\n",
            "# Adam = lr 0.0008 to 1e-05, warmup 0 steps, beta (0.9, 0.99), eps 1e-08\n",
            "#\n",
            "# Found torch 1.13.1+cu116, recommend 1.12.1+cu116 or newer\n",
            "# Found deepspeed 0.8.0, recommend 0.7.0 (faster than newer versions)\n",
            "# Found pytorch_lightning 1.9.0, recommend 1.7.4 or newer\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "{'data_order': '', 'load_model': '', 'wandb': 'pho-vlc', 'proj_dir': 'out', 'random_seed': -1, 'data_file': 'vlc.txt', 'data_type': 'utf-8', 'vocab_size': 0, 'ctx_len': 192, 'epoch_steps': 2000, 'epoch_count': 20, 'epoch_begin': 0, 'epoch_save': 5, 'micro_bsz': 64, 'n_layer': 5, 'n_embd': 320, 'pre_ffn': 0, 'head_qk': 0, 'tiny_att_dim': 0, 'tiny_att_layer': -999, 'lr_init': 0.0008, 'lr_final': 1e-05, 'warmup_steps': 0, 'beta1': 0.9, 'beta2': 0.99, 'adam_eps': 1e-08, 'grad_cp': 0, 'my_pile_stage': 0, 'my_pile_shift': -1, 'my_pile_edecay': 0, 'layerwise_lr': 1, 'ds_bucket_mb': 200, 'my_sample_len': 0, 'my_ffn_shift': 1, 'my_att_shift': 1, 'my_pos_emb': 0, 'load_partial': 0, 'magic_prime': 0, 'logger': False, 'enable_checkpointing': False, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': None, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 100000000000000000000, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': -1, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 100000000000000000000, 'accelerator': 'gpu', 'strategy': 'ddp_find_unused_parameters_false', 'sync_batchnorm': False, 'precision': 'fp16', 'enable_model_summary': True, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': None, 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True, 'my_timestamp': '2023-02-20-13-24-48', 'betas': (0.9, 0.99), 'real_bsz': 64, 'run_name': '0 ctx192 L5 D320'}\n",
            "\n",
            "\n",
            "\n",
            "Note: you are using fp16 (might overflow). Try bf16 / tf32 for stable training.\n",
            "\n",
            "\n",
            "Current vocab size = 64256 (make sure it's correct)\n",
            "Data has 2771041 samples.\n",
            "Using /root/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py38_cu116/wkv_192...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu116/wkv_192/build.ninja...\n",
            "Building extension module wkv_192...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_192 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 -DTmax=192 -std=c++14 -c /content/symato/rwkv-v4neo/wkv_cuda.cu -o wkv_cuda.cuda.o \n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_PS0_S3_S3_S3_\n",
            "    2304 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 43 registers, 440 bytes cmem[0], 8 bytes cmem[2]\n",
            "ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 40 registers, 408 bytes cmem[0]\n",
            "[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_192 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /content/symato/rwkv-v4neo/wkv_op.cpp -o wkv_op.o \n",
            "[3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_192.so\n",
            "Loading extension module wkv_192...\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# Init model weight (slow for large models)...\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "64256 320   -0.0008 emb.weight\n",
            "320   320   0    blocks.0.att.key.weight\n",
            "320   320   1.0  blocks.0.att.value.weight\n",
            "320   320   0    blocks.0.att.receptance.weight\n",
            "320   320   0    blocks.0.att.output.weight\n",
            "1280  320   1.0  blocks.0.ffn.key.weight\n",
            "320   320   0    blocks.0.ffn.receptance.weight\n",
            "320   1280  0    blocks.0.ffn.value.weight\n",
            "320   320   0    blocks.1.att.key.weight\n",
            "320   320   1.0  blocks.1.att.value.weight\n",
            "320   320   0    blocks.1.att.receptance.weight\n",
            "320   320   0    blocks.1.att.output.weight\n",
            "1280  320   1.0  blocks.1.ffn.key.weight\n",
            "320   320   0    blocks.1.ffn.receptance.weight\n",
            "320   1280  0    blocks.1.ffn.value.weight\n",
            "320   320   0    blocks.2.att.key.weight\n",
            "320   320   1.0  blocks.2.att.value.weight\n",
            "320   320   0    blocks.2.att.receptance.weight\n",
            "320   320   0    blocks.2.att.output.weight\n",
            "1280  320   1.0  blocks.2.ffn.key.weight\n",
            "320   320   0    blocks.2.ffn.receptance.weight\n",
            "320   1280  0    blocks.2.ffn.value.weight\n",
            "320   320   0    blocks.3.att.key.weight\n",
            "320   320   1.0  blocks.3.att.value.weight\n",
            "320   320   0    blocks.3.att.receptance.weight\n",
            "320   320   0    blocks.3.att.output.weight\n",
            "1280  320   1.0  blocks.3.ffn.key.weight\n",
            "320   320   0    blocks.3.ffn.receptance.weight\n",
            "320   1280  0    blocks.3.ffn.value.weight\n",
            "320   320   0    blocks.4.att.key.weight\n",
            "320   320   1.0  blocks.4.att.value.weight\n",
            "320   320   0    blocks.4.att.receptance.weight\n",
            "320   320   0    blocks.4.att.output.weight\n",
            "1280  320   1.0  blocks.4.ffn.key.weight\n",
            "320   320   0    blocks.4.ffn.receptance.weight\n",
            "320   1280  0    blocks.4.ffn.value.weight\n",
            "64256 320   0.5  head.weight\n",
            "Save to out/rwkv-init.pth...\n",
            "########## Loading out/rwkv-init.pth... ##########\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "64256 320   emb.weight\n",
            "320         blocks.0.ln1.weight\n",
            "320         blocks.0.ln1.bias\n",
            "320         blocks.0.ln2.weight\n",
            "320         blocks.0.ln2.bias\n",
            "320         blocks.0.ln0.weight\n",
            "320         blocks.0.ln0.bias\n",
            "320         blocks.0.att.time_decay\n",
            "320         blocks.0.att.time_first\n",
            "320         blocks.0.att.time_mix_k\n",
            "320         blocks.0.att.time_mix_v\n",
            "320         blocks.0.att.time_mix_r\n",
            "320   320   blocks.0.att.key.weight\n",
            "320   320   blocks.0.att.value.weight\n",
            "320   320   blocks.0.att.receptance.weight\n",
            "320   320   blocks.0.att.output.weight\n",
            "320         blocks.0.ffn.time_mix_k\n",
            "320         blocks.0.ffn.time_mix_r\n",
            "1280  320   blocks.0.ffn.key.weight\n",
            "320   320   blocks.0.ffn.receptance.weight\n",
            "320   1280  blocks.0.ffn.value.weight\n",
            "320         blocks.1.ln1.weight\n",
            "320         blocks.1.ln1.bias\n",
            "320         blocks.1.ln2.weight\n",
            "320         blocks.1.ln2.bias\n",
            "320         blocks.1.att.time_decay\n",
            "320         blocks.1.att.time_first\n",
            "320         blocks.1.att.time_mix_k\n",
            "320         blocks.1.att.time_mix_v\n",
            "320         blocks.1.att.time_mix_r\n",
            "320   320   blocks.1.att.key.weight\n",
            "320   320   blocks.1.att.value.weight\n",
            "320   320   blocks.1.att.receptance.weight\n",
            "320   320   blocks.1.att.output.weight\n",
            "320         blocks.1.ffn.time_mix_k\n",
            "320         blocks.1.ffn.time_mix_r\n",
            "1280  320   blocks.1.ffn.key.weight\n",
            "320   320   blocks.1.ffn.receptance.weight\n",
            "320   1280  blocks.1.ffn.value.weight\n",
            "320         blocks.2.ln1.weight\n",
            "320         blocks.2.ln1.bias\n",
            "320         blocks.2.ln2.weight\n",
            "320         blocks.2.ln2.bias\n",
            "320         blocks.2.att.time_decay\n",
            "320         blocks.2.att.time_first\n",
            "320         blocks.2.att.time_mix_k\n",
            "320         blocks.2.att.time_mix_v\n",
            "320         blocks.2.att.time_mix_r\n",
            "320   320   blocks.2.att.key.weight\n",
            "320   320   blocks.2.att.value.weight\n",
            "320   320   blocks.2.att.receptance.weight\n",
            "320   320   blocks.2.att.output.weight\n",
            "320         blocks.2.ffn.time_mix_k\n",
            "320         blocks.2.ffn.time_mix_r\n",
            "1280  320   blocks.2.ffn.key.weight\n",
            "320   320   blocks.2.ffn.receptance.weight\n",
            "320   1280  blocks.2.ffn.value.weight\n",
            "320         blocks.3.ln1.weight\n",
            "320         blocks.3.ln1.bias\n",
            "320         blocks.3.ln2.weight\n",
            "320         blocks.3.ln2.bias\n",
            "320         blocks.3.att.time_decay\n",
            "320         blocks.3.att.time_first\n",
            "320         blocks.3.att.time_mix_k\n",
            "320         blocks.3.att.time_mix_v\n",
            "320         blocks.3.att.time_mix_r\n",
            "320   320   blocks.3.att.key.weight\n",
            "320   320   blocks.3.att.value.weight\n",
            "320   320   blocks.3.att.receptance.weight\n",
            "320   320   blocks.3.att.output.weight\n",
            "320         blocks.3.ffn.time_mix_k\n",
            "320         blocks.3.ffn.time_mix_r\n",
            "1280  320   blocks.3.ffn.key.weight\n",
            "320   320   blocks.3.ffn.receptance.weight\n",
            "320   1280  blocks.3.ffn.value.weight\n",
            "320         blocks.4.ln1.weight\n",
            "320         blocks.4.ln1.bias\n",
            "320         blocks.4.ln2.weight\n",
            "320         blocks.4.ln2.bias\n",
            "320         blocks.4.att.time_decay\n",
            "320         blocks.4.att.time_first\n",
            "320         blocks.4.att.time_mix_k\n",
            "320         blocks.4.att.time_mix_v\n",
            "320         blocks.4.att.time_mix_r\n",
            "320   320   blocks.4.att.key.weight\n",
            "320   320   blocks.4.att.value.weight\n",
            "320   320   blocks.4.att.receptance.weight\n",
            "320   320   blocks.4.att.output.weight\n",
            "320         blocks.4.ffn.time_mix_k\n",
            "320         blocks.4.ffn.time_mix_r\n",
            "1280  320   blocks.4.ffn.key.weight\n",
            "320   320   blocks.4.ffn.receptance.weight\n",
            "320   1280  blocks.4.ffn.value.weight\n",
            "320         ln_out.weight\n",
            "320         ln_out.bias\n",
            "64256 320   head.weight\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type       | Params\n",
            "--------------------------------------\n",
            "0 | emb    | Embedding  | 20.6 M\n",
            "1 | blocks | ModuleList | 6.7 M \n",
            "2 | ln_out | LayerNorm  | 640   \n",
            "3 | head   | Linear     | 20.6 M\n",
            "--------------------------------------\n",
            "47.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "47.8 M    Total params\n",
            "95.597    Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/2000 [00:00<?, ?it/s] Login to wandb...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_manager.py\", line 151, in _teardown\n",
            "    self._inform_teardown(exit_code)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_manager.py\", line 191, in _inform_teardown\n",
            "    svc_iface._svc_inform_teardown(exit_code)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/service/service_sock.py\", line 68, in _svc_inform_teardown\n",
            "    self._sock_client.send(inform_teardown=inform_teardown)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py\", line 211, in send\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/sock_client.py\", line 148, in _send_message\n",
            "    data = msg.SerializeToString()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsbKvAgJ-Hw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}