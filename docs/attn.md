https://github.com/lucidrains/attention

Recently, three papers have concurrently closed in on a connection between self-attention and gradient descent, while investigating in-context learning properties of Transformers!
- Transformers learn in-context by gradient descent
- What learning algorithm is in-context learning? Investigations with linear models
- Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers

